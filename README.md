# Реализация Direct Preference Optimization

В данном проекте представлено решение тестового задания по ручной реализации алгоритма DPO на основе статьи "Direct Preference Optimization: Your Language Model is Secretly a Reward Model".

## Структура проекта

```text
.
├── Dockerfile          
├── requirements.txt  
├── theory.pdf          # решение теоретической части с кратким изложением статьи, анализом преимуществ DPO и разбором off-policy постановки задачи
└── src/
    ├── main.py         # точка входа для запуска всего пайплайна
    ├── data.py         # логика загрузки датасета и токенизации
    ├── model.py        # инициализация Policy-модели и замороженной Reference-модели
    ├── dpo_loss.py     # кастомная математическая реализация функции потерь DPO
    └── training.py     # цикл обучения
```

## Инструкция по воспроизведению результатов

### Запуск через Docker
1. Собираем Docker-образ:
   ```bash
   docker build -t dpo_project .
   ```
2. Запускаем контейнер на сервере с GPU:
   ```bash
    docker run --gpus all -it dpo_project
    ```
### Локальная установка
1. Устанавливаем зависимости:
    ```
   pip install -r requirements.txt
   ```
2. Запускаем обучение:
     ```
    python -m src.main
   ```

## Анализ результатов

Для оценки качества работы алгоритма DPO мы логировали функцию потерь и неявную разницу ревордов (Implicit Reward Margin) между выбранными и отклоненными ответами.

### 1. Количественные метрики
![Training Metrics](training_metrics.png)

* **Loss:** Как видно на графике, функция потерь DPO успешно оптимизируется на протяжении эпохи.
* **Reward Margin:** Разница между ревордами (Chosen - Rejected) стабильно растет, что математически подтверждает: модель учится присваивать бóльшую вероятность предпочтительным ответам и меньшую — отклоненным.

### 2. Качественный анализ (Примеры генерации)
Для проверки поведения модели "в дикой природе" мы сравнили ответы базовой GPT-2 и нашей DPO-модели на отложенных промптах.

**Промпт:** `\n\nHuman: What are the main differences between Python and C++?\n\nAssistant:`
* **Базовая GPT-2:** `[Вставь сырой вывод из inference.py]`
* **DPO GPT-2:** `[Вставь вывод DPO модели из inference.py]`
* **Вывод:** `[Напиши свои наблюдения: стала ли модель лучше выдерживать структуру диалога и не "сыпаться" в случайную генерацию?]`
